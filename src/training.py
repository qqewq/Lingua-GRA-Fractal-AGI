"""
training.py ‚Äì –æ–±—É—á–µ–Ω–∏–µ Lingua GRA —Å GRA-–ø–µ–Ω–æ–π –∏ —Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω—ã–º —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ç–æ—Ä–æ–º

üá∑üá∫
–ó–¥–µ—Å—å –æ–ø–∏—Å–∞–Ω –±–∞–∑–æ–≤—ã–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π —Ü–∏–∫–ª:
- –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π (—Å–∏–º–≤–æ–ª—å–Ω—ã–π, —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π, –ø—Ä–∞–≥–º–∞—Ç–∏—á–µ—Å–∫–∏–π);
- –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ GRA-–ø–µ–Ω—ã (foam) –æ—Ç –ø—Ä–æ–µ–∫—Ç–æ—Ä–æ–≤;
- –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–æ–≥–æ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ç–æ—Ä–∞ –ø–æ D2 (correlation dimension).

üá¨üáß
This module implements a basic training loop:
- combining losses from different levels (symbolic, semantic, pragmatic);
- adding GRA foam from projectors;
- adding a fractal regularizer based on D2 (correlation dimension).
"""

from __future__ import annotations

from typing import Dict, Tuple

import torch
import torch.nn as nn
import torch.optim as optim

from .gra_core import GRAFunctional
from .fractal_utils import fractal_regularizer
from .language_levels import BaseLevel, Level
from .neural_encoders import reconstruction_loss_logits, PragmaticPolicy


class LinguaGRAModel(nn.Module):
    """
    üá∑–ì–£ –í—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è –º–æ–¥–µ–ª—å, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è —É—Ä–æ–≤–Ω–∏ Lingua GRA.

    –°–æ–¥–µ—Ä–∂–∏—Ç:
    - —Å–ª–æ–≤–∞—Ä—å —É—Ä–æ–≤–Ω–µ–π (BaseLevel),
    - –ø—Ä–∞–≥–º–∞—Ç–∏—á–µ—Å–∫–∏–π policy-–º–æ–¥—É–ª—å (–µ—Å–ª–∏ –Ω—É–∂–µ–Ω),
    - GRAFunctional –¥–ª—è –ø–µ–Ω—ã + —ç–∫—Å—Ç—Ä–∞-—Ç–µ—Ä–º–æ–≤.

    üá¨üáß High-level model that glues together Lingua GRA levels.

    Holds:
    - dict of levels (BaseLevel),
    - pragmatic policy module (optionally),
    - GRAFunctional for foam + extra terms.
    """

    def __init__(
        self,
        levels: Dict[Level, BaseLevel],
        policy: PragmaticPolicy | None,
        gra: GRAFunctional,
    ):
        super().__init__()
        self.levels = nn.ModuleDict({str(l): lvl for l, lvl in levels.items()})
        self.policy = policy
        self.gra = gra

    def forward_token_level(self, token_ids: torch.Tensor):
        """
        üá∑–ì–£ –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ —Å–∏–º–≤–æ–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å.

        üá¨üáß Forward pass through the symbolic level.
        """
        lvl = self.levels[str(Level.SYMBOLIC)]
        h, h_proj, logits = lvl(token_ids)
        return h, h_proj, logits

    def forward_semantic_level(self, sent_emb: torch.Tensor):
        """
        üá∑–ì–£ –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å.

        üá¨üáß Forward pass through the semantic level.
        """
        lvl = self.levels[str(Level.SEMANTIC)]
        h, h_proj, _ = lvl(sent_emb)
        return h, h_proj

    def act_pragmatic(self, obs: torch.Tensor, msg_emb: torch.Tensor) -> torch.Tensor:
        """
        üá∑–ì–£ –í—ã—á–∏—Å–ª–∏—Ç—å logits –¥–µ–π—Å—Ç–≤–∏–π –Ω–∞ –ø—Ä–∞–≥–º–∞—Ç–∏—á–µ—Å–∫–æ–º —É—Ä–æ–≤–Ω–µ.

        üá¨üáß Compute action logits at the pragmatic level.
        """
        if self.policy is None:
            raise RuntimeError("Pragmatic policy is not set.")
        return self.policy(obs, msg_emb)


# ---------------------------------------------------------------------------
# –ü—Ä–∏–º–µ—Ä –æ–¥–Ω–æ–≥–æ —à–∞–≥–∞ –æ–±—É—á–µ–Ω–∏—è
# ---------------------------------------------------------------------------


def train_step_supervised_token_semantic(
    model: LinguaGRAModel,
    optimizer: optim.Optimizer,
    batch_tokens: torch.Tensor,
    batch_target_tokens: torch.Tensor,
    semantic_inputs: torch.Tensor,
    semantic_target_dim: float | None = None,
    gamma_fract_semantic: float = 0.0,
) -> Dict[str, float]:
    """
    üá∑–ì–£
    –û–¥–∏–Ω–æ—á–Ω—ã–π —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ø—Ä–æ—â—ë–Ω–Ω–æ–≥–æ —Å—Ü–µ–Ω–∞—Ä–∏—è:
    - —Å–∏–º–≤–æ–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å: —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤ + GRA-–ø–µ–Ω–∞;
    - —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å: —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è GRA + (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) —Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω—ã–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ç–æ—Ä.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    - batch_tokens: [B, T] ‚Äì –≤—Ö–æ–¥–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã;
    - batch_target_tokens: [B] –∏–ª–∏ [B, T] ‚Äì —Ü–µ–ª–∏ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏;
    - semantic_inputs: [B, D_in] ‚Äì –≤—Ö–æ–¥—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, pooled embeddings);
    - semantic_target_dim: —Ü–µ–ª–µ–≤–∞—è D2; –µ—Å–ª–∏ None, —Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω—ã–π loss –Ω–µ –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è;
    - gamma_fract_semantic: –≤–µ—Å —Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–æ–≥–æ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ç–æ—Ä–∞.

    üá¨üáß
    Single training step for a simplified scenario:
    - symbolic level: token reconstruction + GRA foam;
    - semantic level: GRA regularization + optional fractal regularizer.
    """
    model.train()
    optimizer.zero_grad()

    logs: Dict[str, float] = {}

    # --- –°–∏–º–≤–æ–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å -------------------------------------------------
    h_sym, h_sym_proj, logits = model.forward_token_level(batch_tokens)

    # –ü—Ä–∏–º–µ—Ä: –µ—Å–ª–∏ —Ü–µ–ª—å ‚Äî –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –æ–¥–∏–Ω —Ç–æ–∫–µ–Ω (–Ω–∞–ø—Ä–∏–º–µ—Ä, next token),
    # —Ç–æ batch_target_tokens: [B]
    recon_loss_sym = reconstruction_loss_logits(logits, batch_target_tokens)
    logs["recon_sym"] = float(recon_loss_sym.detach().cpu())

    foam_sym = ((h_sym - h_sym_proj) ** 2).mean()
    logs["foam_sym"] = float(foam_sym.detach().cpu())

    # --- –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å ---------------------------------------------
    h_sem, h_sem_proj = model.forward_semantic_level(semantic_inputs)
    foam_sem = ((h_sem - h_sem_proj) ** 2).mean()
    logs["foam_sem"] = float(foam_sem.detach().cpu())

    fract_loss_sem = 0.0
    D2_sem_val = 0.0
    if semantic_target_dim is not None and gamma_fract_semantic > 0.0:
        fract_loss_tensor, D2_sem = fractal_regularizer(
            h_sem, target_dim=semantic_target_dim, weight=gamma_fract_semantic
        )
        fract_loss_sem = fract_loss_tensor
        D2_sem_val = float(D2_sem.detach().cpu())
        logs["fract_sem"] = float(fract_loss_tensor.detach().cpu())
        logs["D2_sem"] = D2_sem_val

    # --- GRA-—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª –ø–æ —É—Ä–æ–≤–Ω—è–º -----------------------------------------
    level_embeddings = {
        int(Level.SYMBOLIC): h_sym,
        int(Level.SEMANTIC): h_sem,
    }
    gra_loss, gra_logs = model.gra(level_embeddings)
    for k, v in gra_logs.items():
        logs[f"gra_{k}"] = v

    # --- –°—É–º–º–∞—Ä–Ω—ã–π loss -----------------------------------------------------
    total_loss = recon_loss_sym + foam_sym + foam_sem + gra_loss
    if semantic_target_dim is not None and gamma_fract_semantic > 0.0:
        total_loss = total_loss + fract_loss_sem

    logs["total_loss"] = float(total_loss.detach().cpu())

    total_loss.backward()
    optimizer.step()

    return logs


# ---------------------------------------------------------------------------
# –ü—Ä–∏–º–µ—Ä —Ü–∏–∫–ª–∞ RL –¥–ª—è –ø—Ä–∞–≥–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è (–Ω–∞–±—Ä–æ—Å–æ–∫)
# ---------------------------------------------------------------------------


def policy_gradient_update(
    model: LinguaGRAModel,
    optimizer: optim.Optimizer,
    obs_batch: torch.Tensor,
    msg_emb_batch: torch.Tensor,
    actions_batch: torch.Tensor,
    returns_batch: torch.Tensor,
) -> Dict[str, float]:
    """
    üá∑–ì–£
    –ü—Ä–æ—Å—Ç–µ–π—à–∞—è REINFORCE-–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–ª—è –ø—Ä–∞–≥–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è.[web:177][web:180]

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    - obs_batch: [B, D_obs] ‚Äì –Ω–∞–±–ª—é–¥–µ–Ω–∏—è,
    - msg_emb_batch: [B, D_msg] ‚Äì —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–æ–±—â–µ–Ω–∏–π,
    - actions_batch: [B] ‚Äì –∏–Ω–¥–µ–∫—Å—ã –¥–µ–π—Å—Ç–≤–∏–π,
    - returns_batch: [B] ‚Äì –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è.

    üá¨üáß
    Simple REINFORCE-style update for the pragmatic level.
    """
    model.train()
    optimizer.zero_grad()

    logits = model.act_pragmatic(obs_batch, msg_emb_batch)  # [B, A]
    log_probs = torch.log_softmax(logits, dim=-1)
    chosen_log_probs = log_probs.gather(1, actions_batch.unsqueeze(1)).squeeze(1)

    # REINFORCE: loss = -E[ return * log pi(a|s) ]
    loss = -(returns_batch * chosen_log_probs).mean()

    loss.backward()
    optimizer.step()

    return {"pg_loss": float(loss.detach().cpu())}
