"""
neural_encoders.py â€“ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ğµ Ğ±Ğ»Ğ¾ĞºĞ¸ Ğ´Ğ»Ñ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ Lingua GRA

ğŸ‡·ğŸ‡º
Ğ—Ğ´ĞµÑÑŒ Ğ½Ğ°Ñ…Ğ¾Ğ´ÑÑ‚ÑÑ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ encoder/decoder-Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ°:
- TokenEncoder / TokenDecoder: ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ;
- SentenceEncoder: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ (Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ/Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚);
- PragmaticPolicy: Ğ¿Ñ€Ğ°Ğ³Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ (policy-ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°);
- ÑƒÑ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.

ğŸ‡¬ğŸ‡§
This module provides basic encoder/decoder modules for different language levels:
- TokenEncoder / TokenDecoder: symbolic level;
- SentenceEncoder: semantic level (sentence/document);
- PragmaticPolicy: pragmatic level (agent policy network);
- utilities to assemble simple models.
"""

from __future__ import annotations

from typing import Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F


# ---------------------------------------------------------------------------
# Ğ¡Ğ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¹ / Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ
# ---------------------------------------------------------------------------


class TokenEncoder(nn.Module):
    """
    ğŸ‡·ğŸ‡º Encoder Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²: embedding + Transformer encoder.

    ğŸ‡¬ğŸ‡§ Encoder for token sequences: embedding + Transformer encoder.
    """

    def __init__(
        self,
        vocab_size: int,
        d_model: int,
        n_layers: int = 2,
        n_heads: int = 4,
        max_len: int = 512,
    ):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_embedding = nn.Embedding(max_len, d_model)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=n_heads,
            dim_feedforward=d_model * 4,
            dropout=0.1,
            batch_first=True,
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)

    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:
        """
        token_ids: [B, T] â€“ Ğ¸Ğ½Ğ´ĞµĞºÑÑ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ².

        Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚:
        - h_cls: [B, D] â€“ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
          (Ğ¿Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ñ [CLS]-Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ¼ Ğ² Transformer-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…).[web:162][web:166]
        """
        bsz, seq_len = token_ids.shape
        pos_ids = torch.arange(seq_len, device=token_ids.device).unsqueeze(0).expand(bsz, -1)

        x = self.embedding(token_ids) + self.pos_embedding(pos_ids)  # [B, T, D]
        h = self.encoder(x)  # [B, T, D]

        # Ğ¿Ñ€Ğ¾ÑÑ‚Ğ°Ñ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ: Ğ±ĞµÑ€Ñ‘Ğ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ ĞºĞ°Ğº "CLS"
        h_cls = h[:, 0, :]
        return h_cls


class TokenDecoder(nn.Module):
    """
    ğŸ‡·ğŸ‡º Decoder: Ğ¸Ğ· ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾ Ğ² Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ.

    ğŸ‡¬ğŸ‡§ Decoder: from sequence embedding back to vocabulary logits.
    """

    def __init__(self, vocab_size: int, d_model: int):
        super().__init__()
        self.linear = nn.Linear(d_model, vocab_size)

    def forward(self, h: torch.Tensor) -> torch.Tensor:
        """
        h: [B, D] â†’ logits: [B, V]

        ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ softmax/ÑÑĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³Ğ°.
        """
        return self.linear(h)


# ---------------------------------------------------------------------------
# Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ (Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ/Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚)
# ---------------------------------------------------------------------------


class SentenceEncoder(nn.Module):
    """
    ğŸ‡·Ğ“Ğ£ Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ encoder Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹/Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ².

    Ğ’ Ğ¿Ñ€Ğ¾ÑÑ‚ĞµĞ¹ÑˆĞµĞ¼ Ğ²Ğ¸Ğ´Ğµ:
    - Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ ÑƒĞ¶Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğµ sentence-ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, ÑÑ€ĞµĞ´Ğ½ĞµĞµ Ğ¿Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼),
      Ğ¸ Ğ´Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¸Ñ… MLP-ÑĞ»Ğ¾ĞµĞ¼;
    Ğ¸Ğ»Ğ¸
    - Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ²ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ TokenEncoder Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ.

    ğŸ‡¬ğŸ‡§ Semantic encoder for sentences/documents.

    In the simplest form:
    - takes pre-computed sentence embeddings (e.g., mean-pooled token embeddings),
      and refines them with an MLP;
    or
    - can include a TokenEncoder and aggregate internally.
    """

    def __init__(self, in_dim: int, d_model: int):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, d_model * 2),
            nn.ReLU(),
            nn.Linear(d_model * 2, d_model),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: [B, D_in] â†’ [B, D_model]
        """
        return self.net(x)


# ---------------------------------------------------------------------------
# ĞŸÑ€Ğ°Ğ³Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ: policy-ÑĞµÑ‚ÑŒ
# ---------------------------------------------------------------------------


class PragmaticPolicy(nn.Module):
    """
    ğŸ‡·Ğ“Ğ£ Policy-ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°Ğ³Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ.

    Ğ’Ñ…Ğ¾Ğ´:
    - obs: Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ ÑÑ€ĞµĞ´Ñ‹ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ grid-world),
    - msg: ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° (ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ).

    Ğ’Ñ‹Ñ…Ğ¾Ğ´:
    - logits Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹.

    ğŸ‡¬ğŸ‡§ Policy network for the pragmatic level.

    Input:
    - obs: environment observation (e.g., grid-world state),
    - msg: message embedding from another agent (semantic level).

    Output:
    - action logits.
    """

    def __init__(self, obs_dim: int, msg_dim: int, hidden_dim: int, n_actions: int):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_dim + msg_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, n_actions),
        )

    def forward(self, obs: torch.Tensor, msg: torch.Tensor) -> torch.Tensor:
        """
        obs: [B, D_obs]
        msg: [B, D_msg]
        returns: logits [B, n_actions]
        """
        x = torch.cat([obs, msg], dim=-1)
        return self.net(x)


# ---------------------------------------------------------------------------
# Ğ’ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
# ---------------------------------------------------------------------------


class SemanticLevelModel(nn.Module):
    """
    ğŸ‡·Ğ“Ğ£ Ğ£Ğ¿Ñ€Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ:
    - sentence encoder + projector (Ğ·Ğ°Ğ´Ğ°Ñ‘Ñ‚ÑÑ ÑĞ½Ğ°Ñ€ÑƒĞ¶Ğ¸) + (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾) decoder.

    ğŸ‡¬ğŸ‡§ Simplified semantic level model:
    - sentence encoder + external projector + optional decoder.
    """

    def __init__(
        self,
        encoder: SentenceEncoder,
        projector: nn.Module,
        decoder: nn.Module | None = None,
    ):
        super().__init__()
        self.encoder = encoder
        self.projector = projector
        self.decoder = decoder

    def forward(self, x: torch.Tensor):
        """
        x: [B, D_in] â€“ sentence-level inputs (e.g., pooled token embeddings).

        Returns:
        - h: [B, D] â€“ raw embeddings,
        - h_proj: [B, D] â€“ projected embeddings,
        - x_rec: [B, D_out] or None â€“ reconstructed outputs if decoder is present.
        """
        h = self.encoder(x)
        h_proj = self.projector(h)
        x_rec = self.decoder(h_proj) if self.decoder is not None else None
        return h, h_proj, x_rec


class TokenLevelModel(nn.Module):
    """
    ğŸ‡·Ğ“Ğ£ Ğ£Ğ¿Ñ€Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ:
    - TokenEncoder + projector + TokenDecoder.

    ğŸ‡¬ğŸ‡§ Simplified token-level model:
    - TokenEncoder + projector + TokenDecoder.
    """

    def __init__(
        self,
        encoder: TokenEncoder,
        projector: nn.Module,
        decoder: TokenDecoder,
    ):
        super().__init__()
        self.encoder = encoder
        self.projector = projector
        self.decoder = decoder

    def forward(self, token_ids: torch.Tensor):
        """
        token_ids: [B, T]

        Returns:
        - h: [B, D],
        - h_proj: [B, D],
        - logits: [B, V]
        """
        h = self.encoder(token_ids)
        h_proj = self.projector(h)
        logits = self.decoder(h_proj)
        return h, h_proj, logits


# ---------------------------------------------------------------------------
# ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ
# ---------------------------------------------------------------------------


def reconstruction_loss_logits(
    logits: torch.Tensor,
    target_ids: torch.Tensor,
) -> torch.Tensor:
    """
    ğŸ‡·Ğ“Ğ£ ĞšÑ€Ğ¾ÑÑ-ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ñ‹Ğ¹ loss Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ğ¾ logits.

    ğŸ‡¬ğŸ‡§ Cross-entropy loss for token reconstruction from logits.
    """
    # logits: [B, V], target_ids: [B]
    return F.cross_entropy(logits, target_ids)
