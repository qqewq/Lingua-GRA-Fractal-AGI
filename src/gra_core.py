"""
gra_core.py ‚Äì GRA —è–¥—Ä–æ –¥–ª—è Lingua GRA

üá∑üá∫
–ó–¥–µ—Å—å –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏:
- HilbertSpace: –æ–±–æ–ª–æ—á–∫–∞ –Ω–∞–¥ –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π;
- Projector: –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞ –Ω–∞ –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Ü–µ–ª–µ–π G_l;
- GRAFunctional: –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª ¬´–ø–µ–Ω—ã¬ª –¥–ª—è –æ–±–Ω—É–ª—ë–Ω–∫–∏.

üá¨üáß
This module defines:
- HilbertSpace: a thin wrapper over the representation vector space;
- Projector: an approximation of the goal projector P_{G_l};
- GRAFunctional: the multi-level foam functional for annihilation.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Callable, Dict, List, Optional, Tuple

import torch
import torch.nn as nn


@dataclass
class HilbertSpace:
    """
    üá∑üá∫ –ê–±—Å—Ç—Ä–∞–∫—Ü–∏—è –≥–∏–ª—å–±–µ—Ä—Ç–æ–≤–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π.

    –ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ:
    - —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å d,
    - —Ç–∏–ø –Ω–æ—Å–∏—Ç–µ–ª—è (–≤ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç: torch.Tensor),
    - —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –Ω–æ—Ä–º—ã –∏ —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è.

    üá¨üáß Abstraction of a Hilbert space of representations.

    In practice this is:
    - dimension d,
    - carrier type (currently: torch.Tensor),
    - functions for norm and inner product.
    """

    dim: int

    def inner(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        """–°–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ / Inner product."""
        return (x * y).sum(dim=-1)

    def norm2(self, x: torch.Tensor) -> torch.Tensor:
        """–ö–≤–∞–¥—Ä–∞—Ç –Ω–æ—Ä–º—ã / Squared norm."""
        return self.inner(x, x)


class Projector(nn.Module):
    """
    üá∑üá∫ –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π –ø—Ä–æ–µ–∫—Ç–æ—Ä, –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä—É—é—â–∏–π –æ–ø–µ—Ä–∞—Ç–æ—Ä P_G.

    –ò–¥–µ—è:
    - forward(x) ‚âà P_G x
    - foam(x) = ||(1 - P_G) x||^2

    üá¨üáß Neural projector approximating the operator P_G.

    Idea:
    - forward(x) ‚âà P_G x
    - foam(x) = ||(1 - P_G) x||^2
    """

    def __init__(self, dim: int, hidden_mult: int = 2):
        super().__init__()
        h = dim * hidden_mult
        self.net = nn.Sequential(
            nn.Linear(dim, h),
            nn.ReLU(),
            nn.Linear(h, dim),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.net(x)

    def foam(self, x: torch.Tensor) -> torch.Tensor:
        """
        üá∑üá∫ –ü–µ–Ω—É —Å—á–∏—Ç–∞–µ–º –∫–∞–∫ —Å—Ä–µ–¥–Ω–∏–π –∫–≤–∞–¥—Ä–∞—Ç –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è (x - P_G x).
        üá¨üáß Foam is the mean squared deviation (x - P_G x).
        """
        x_proj = self.forward(x)
        return ((x - x_proj) ** 2).mean()


class GRAFunctional(nn.Module):
    """
    üá∑üá∫ GRA-—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª –¥–ª—è –Ω–∞–±–æ—Ä–∞ —É—Ä–æ–≤–Ω–µ–π.

    –•—Ä–∞–Ω–∏—Ç:
    - —Å–ª–æ–≤–∞—Ä—å –ø—Ä–æ–µ–∫—Ç–æ—Ä–æ–≤ –ø–æ —É—Ä–æ–≤–Ω—è–º,
    - –≤–µ—Å–∞ Lambda_l,
    - –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ç–æ—Ä—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω—ã–π).

    foam_terms[l](x_l) –æ–∂–∏–¥–∞–µ—Ç—Å—è –∫–∞–∫ scalar-—Ç–µ–Ω–∑–æ—Ä (loss).

    üá¨üáß GRA functional over multiple levels.

    Stores:
    - a dict of projectors per level,
    - weights Lambda_l,
    - optional extra regularizers (e.g. fractal term).

    foam_terms[l](x_l) is expected to be a scalar tensor (loss).
    """

    def __init__(
        self,
        projectors: Dict[int, Projector],
        lambdas: Dict[int, float],
        extra_terms: Optional[
            Dict[int, List[Callable[[torch.Tensor], torch.Tensor]]]
        ] = None,
    ):
        super().__init__()
        self.projectors = nn.ModuleDict(
            {str(l): p for l, p in projectors.items()}
        )
        self.lambdas = lambdas
        self.extra_terms = extra_terms or {}

    def forward(
        self,
        level_embeddings: Dict[int, torch.Tensor],
    ) -> Tuple[torch.Tensor, Dict[str, float]]:
        """
        üá∑üá∫
        level_embeddings: —Å–ª–æ–≤–∞—Ä—å {l: x_l}, –≥–¥–µ x_l ‚Äî –±–∞—Ç—á –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —É—Ä–æ–≤–Ω—è l.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        - —Å—É–º–º–∞—Ä–Ω—ã–π loss J_GRA,
        - –ª–æ–≥-–º–µ—Ç—Ä–∏–∫–∏ –ø–æ —É—Ä–æ–≤–Ω—è–º.

        üá¨üáß
        level_embeddings: dict {l: x_l}, where x_l is the batch of representations at level l.

        Returns:
        - total loss J_GRA,
        - per-level metrics for logging.
        """
        total_loss = 0.0
        logs: Dict[str, float] = {}

        for l, x_l in level_embeddings.items():
            key = str(l)
            if key not in self.projectors:
                continue

            proj = self.projectors[key]
            foam_l = proj.foam(x_l)
            lam = self.lambdas.get(l, 1.0)

            loss_l = lam * foam_l
            logs[f"foam_l{l}"] = float(foam_l.detach().cpu())

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä —Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω—ã–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ç–æ—Ä
            if l in self.extra_terms:
                for i, term in enumerate(self.extra_terms[l]):
                    term_val = term(x_l)
                    loss_l = loss_l + term_val
                    logs[f"extra_l{l}_{i}"] = float(term_val.detach().cpu())

            total_loss = total_loss + loss_l

        return total_loss, logs
