"""
language_levels.py ‚Äì —É—Ä–æ–≤–Ω–∏ —è–∑—ã–∫–∞ –≤ Lingua GRA

üá∑üá∫
–ó–¥–µ—Å—å –æ–ø–∏—Å–∞–Ω—ã –æ—Å–Ω–æ–≤–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ —è–∑—ã–∫–∞:
- SYMBOLIC: —Ç–æ–∫–µ–Ω—ã, –±–∞–∑–æ–≤—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å;
- SYNTACTIC: –¥–µ—Ä–µ–≤—å—è/—Å—Ç—Ä—É–∫—Ç—É—Ä—ã;
- SEMANTIC: —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–º—ã—Å–ª–æ–≤;
- PRAGMATIC: –∞–≥–µ–Ω—Ç-–≤-—Å—Ä–µ–¥–µ, —Ü–µ–ª–∏ –∏ –ø–ª–∞–Ω—ã;
- META: —è–∑—ã–∫ –æ —è–∑—ã–∫–µ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö.

–ö–∞–∂–¥—ã–π —É—Ä–æ–≤–µ–Ω—å —Å–≤—è–∑—ã–≤–∞–µ—Ç:
- HilbertSpace,
- –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π encoder/decoder,
- –ø—Ä–æ–µ–∫—Ç–æ—Ä P_G.

üá¨üáß
This module defines the main language levels:
- SYMBOLIC: tokens, basic syntax;
- SYNTACTIC: trees/structures;
- SEMANTIC: meaning embeddings;
- PRAGMATIC: agent-in-environment, goals and plans;
- META: language about language and architectures.

Each level ties together:
- a HilbertSpace,
- neural encoder/decoder,
- a projector P_G.
"""

from __future__ import annotations

from dataclasses import dataclass
from enum import IntEnum
from typing import Dict, Optional, Tuple

import torch
import torch.nn as nn

from .gra_core import HilbertSpace, Projector


class Level(IntEnum):
    SYMBOLIC = 0
    SYNTACTIC = 1
    SEMANTIC = 2
    PRAGMATIC = 3
    META = 4


@dataclass
class LevelConfig:
    """
    üá∑üá∫ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —É—Ä–æ–≤–Ω—è —è–∑—ã–∫–∞.

    üá¨üáß Configuration structure for a language level.
    """

    dim: int
    lambda_weight: float = 1.0
    gamma_fractal: float = 0.0
    target_fractal_dim: Optional[float] = None


class BaseLevel(nn.Module):
    """
    üá∑üá∫ –ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å —É—Ä–æ–≤–Ω—è —è–∑—ã–∫–∞ Lingua GRA.

    –°–æ–¥–µ—Ä–∂–∏—Ç:
    - HilbertSpace,
    - encoder,
    - decoder (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ),
    - projector.

    üá¨üáß Base class for a Lingua GRA language level.

    Holds:
    - HilbertSpace,
    - encoder,
    - optional decoder,
    - projector.
    """

    def __init__(
        self,
        level: Level,
        config: LevelConfig,
        encoder: nn.Module,
        projector: Projector,
        decoder: Optional[nn.Module] = None,
    ):
        super().__init__()
        self.level = level
        self.config = config
        self.hilbert = HilbertSpace(dim=config.dim)
        self.encoder = encoder
        self.decoder = decoder
        self.projector = projector

    def encode(self, x: torch.Tensor) -> torch.Tensor:
        return self.encoder(x)

    def decode(self, h: torch.Tensor) -> Optional[torch.Tensor]:
        if self.decoder is None:
            return None
        return self.decoder(h)

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:
        """
        üá∑üá∫
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        - h: —ç–º–±–µ–¥–¥–∏–Ω–≥ —É—Ä–æ–≤–Ω—è,
        - h_proj: –ø—Ä–æ–µ—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ,
        - x_rec: —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å decoder).

        üá¨üáß
        Returns:
        - h: level embedding,
        - h_proj: projected state,
        - x_rec: reconstruction (if decoder is present).
        """
        h = self.encode(x)
        h_proj = self.projector(h)
        x_rec = self.decode(h_proj)
        return h, h_proj, x_rec


# --- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ -----------------------------------------------------


class SymbolicLevel(BaseLevel):
    """
    üá∑üá∫ –°–∏–º–≤–æ–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å (—Ç–æ–∫–µ–Ω—ã, –±–∞–∑–æ–≤—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å).

    üá¨üáß Symbolic level (tokens, basic syntax).
    """

    def __init__(self, vocab_size: int, config: LevelConfig):
        encoder = SymbolicEncoder(vocab_size, config.dim)
        decoder = SymbolicDecoder(vocab_size, config.dim)
        projector = Projector(dim=config.dim)
        super().__init__(Level.SYMBOLIC, config, encoder, projector, decoder)


class SyntacticLevel(BaseLevel):
    """
    üá∑üá∫ –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å (–¥–µ—Ä–µ–≤—å—è/—Å—Ç—Ä—É–∫—Ç—É—Ä—ã).

    üá¨üáß Syntactic level (trees/structures).
    """

    def __init__(self, config: LevelConfig):
        encoder = SyntacticEncoder(config.dim)
        projector = Projector(dim=config.dim)
        # decoder –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω, –Ω–æ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –¥–µ—Ä–µ–≤–∞
        super().__init__(Level.SYNTACTIC, config, encoder, projector, decoder=None)


class SemanticLevel(BaseLevel):
    """
    üá∑üá∫ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å (—ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π/–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤).

    üá¨üáß Semantic level (embeddings of sentences/documents).
    """

    def __init__(self, config: LevelConfig):
        encoder = SemanticEncoder(config.dim)
        projector = Projector(dim=config.dim)
        super().__init__(Level.SEMANTIC, config, encoder, projector, decoder=None)


class PragmaticLevel(BaseLevel):
    """
    üá∑üá∫ –ü—Ä–∞–≥–º–∞—Ç–∏—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å (–∞–≥–µ–Ω—Ç-–≤-—Å—Ä–µ–¥–µ, –ø–ª–∞–Ω—ã).

    üá¨üáß Pragmatic level (agent-in-environment, plans).
    """

    def __init__(self, obs_dim: int, config: LevelConfig):
        encoder = PragmaticEncoder(obs_dim, config.dim)
        projector = Projector(dim=config.dim)
        super().__init__(Level.PRAGMATIC, config, encoder, projector, decoder=None)


class MetaLevel(BaseLevel):
    """
    üá∑üá∫ –ú–µ—Ç–∞-—É—Ä–æ–≤–µ–Ω—å (–æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–∞–≤–∏–ª, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä).

    üá¨üáß Meta level (description of rules, architectures).
    """

    def __init__(self, config: LevelConfig):
        encoder = MetaEncoder(config.dim)
        projector = Projector(dim=config.dim)
        super().__init__(Level.META, config, encoder, projector, decoder=None)


# --- –ü—Ä–æ—Å—Ç–µ–π—à–∏–µ –∑–∞–≥–ª—É—à–∫–∏ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤/–¥–µ–∫–æ–¥–µ—Ä–æ–≤ -------------------------------


class SymbolicEncoder(nn.Module):
    """
    üá∑üá∫ –ü—Ä–æ—Å—Ç–æ–π encoder –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤: embedding + Transformer-encoder.

    üá¨üáß Simple encoder for tokens: embedding + Transformer encoder.
    """

    def __init__(self, vocab_size: int, d_model: int):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=d_model, nhead=4),
            num_layers=2,
        )

    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:
        # token_ids: [B, T]
        x = self.embedding(token_ids)  # [B, T, D]
        x = x.transpose(0, 1)         # [T, B, D]
        h = self.encoder(x)           # [T, B, D]
        return h.mean(dim=0)          # [B, D] ‚Äì —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏


class SymbolicDecoder(nn.Module):
    """
    üá∑üá∫ –ü—Ä–æ—Å—Ç–æ–π decoder: –ø—Ä–æ–µ–∫—Ü–∏—è –∏–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–ª–æ–≤–∞—Ä—é.

    üá¨üáß Simple decoder: projection from embedding to vocabulary distribution.
    """

    def __init__(self, vocab_size: int, d_model: int):
        super().__init__()
        self.linear = nn.Linear(d_model, vocab_size)

    def forward(self, h: torch.Tensor) -> torch.Tensor:
        # h: [B, D]
        logits = self.linear(h)  # [B, V]
        return logits


class SyntacticEncoder(nn.Module):
    """
    üá∑üá∫ –ó–∞–≥–ª—É—à–∫–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–≥–æ encoder‚Äô–∞.

    –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Å—é–¥–∞ –º–æ–∂–µ—Ç –ø—Ä–∏–π—Ç–∏ –ø–∞—Ä—Å–µ—Ä –∏–ª–∏ graph NN,
    –∫–æ–¥–∏—Ä—É—é—â–∏–π –¥–µ—Ä–µ–≤–æ —Ä–∞–∑–±–æ—Ä–∞ –≤ –≤–µ–∫—Ç–æ—Ä.

    üá¨üáß Placeholder for syntactic encoder.

    In a real implementation this could be a parser or graph NN
    encoding a parse tree into a vector.
    """

    def __init__(self, d_model: int):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.ReLU(),
            nn.Linear(d_model, d_model),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.net(x)


class SemanticEncoder(nn.Module):
    """
    üá∑üá∫ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π encoder: –Ω–µ–±–æ–ª—å—à–æ–π Transformer/MLP –∫–∞–∫ –∑–∞–≥–ª—É—à–∫–∞.

    –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ LLM-—ç–Ω–∫–æ–¥–µ—Ä –∏–∑ transformers.

    üá¨üáß Semantic encoder: small Transformer/MLP as a placeholder.

    In a real project, this can be replaced by a LLM encoder.
    """

    def __init__(self, d_model: int):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(d_model, d_model * 2),
            nn.ReLU(),
            nn.Linear(d_model * 2, d_model),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.net(x)


class PragmaticEncoder(nn.Module):
    """
    üá∑–ì–£ –ü—Ä–∞–≥–º–∞—Ç–∏—á–µ—Å–∫–∏–π encoder: –∏–∑ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è —Å—Ä–µ–¥—ã –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —É—Ä–æ–≤–Ω—è.

    üá¨üáß Pragmatic encoder: from environment observation to level state.
    """

    def __init__(self, obs_dim: int, d_model: int):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_dim, d_model * 2),
            nn.ReLU(),
            nn.Linear(d_model * 2, d_model),
        )

    def forward(self, obs: torch.Tensor) -> torch.Tensor:
        return self.net(obs)


class MetaEncoder(nn.Module):
    """
    üá∑–ì–£ –ú–µ—Ç–∞-encoder: –∫–æ–¥–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–∞–≤–∏–ª, –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä.

    üá¨üáß Meta encoder: encodes descriptions of rules, configurations, architectures.
    """

    def __init__(self, d_model: int):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(d_model, d_model * 2),
            nn.ReLU(),
            nn.Linear(d_model * 2, d_model),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.net(x)


# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ —É—Ä–æ–≤–Ω–µ–π ----------------------------------------


def build_default_levels(
    vocab_size: int,
    obs_dim: int,
) -> Dict[Level, BaseLevel]:
    """
    üá∑–ì–£ –°–æ–±—Ä–∞—Ç—å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –Ω–∞–±–æ—Ä —É—Ä–æ–≤–Ω–µ–π –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.

    üá¨üáß Build a default set of levels for experiments.
    """
    levels: Dict[Level, BaseLevel] = {}

    levels[Level.SYMBOLIC] = SymbolicLevel(
        vocab_size=vocab_size,
        config=LevelConfig(dim=128, lambda_weight=1.0),
    )
    levels[Level.SYNTACTIC] = SyntacticLevel(
        config=LevelConfig(dim=128, lambda_weight=0.5),
    )
    levels[Level.SEMANTIC] = SemanticLevel(
        config=LevelConfig(
            dim=256,
            lambda_weight=1.0,
            gamma_fractal=0.1,
            target_fractal_dim=None,  # –º–æ–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ—Å–ª–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        ),
    )
    levels[Level.PRAGMATIC] = PragmaticLevel(
        obs_dim=obs_dim,
        config=LevelConfig(dim=128, lambda_weight=0.5),
    )
    levels[Level.META] = MetaLevel(
        config=LevelConfig(dim=128, lambda_weight=0.1),
    )

    return levels
