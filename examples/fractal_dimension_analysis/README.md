# Fractal Dimension Analysis ‚Äì README

> üá∑üá∫ –ü—Ä–∏–º–µ—Ä –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –ø–æ –æ—Ü–µ–Ω–∫–µ —Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.  
> üá¨üáß Example of a reproducible experiment for estimating the fractal dimension of embeddings.

---

## üá∑üá∫ 1. –û–ø–∏—Å–∞–Ω–∏–µ

–≠—Ç–æ—Ç –ø—Ä–∏–º–µ—Ä –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫:

- –æ–±—É—á–∏—Ç—å (–∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å) word2vec-—ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞;
- –≤—ã–±—Ä–∞—Ç—å –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –∏ –∏–∑–≤–ª–µ—á—å –∏—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è;
- –æ—Ü–µ–Ω–∏—Ç—å **–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å D‚ÇÇ** —ç—Ç–æ–≥–æ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Ç–æ—á–µ–∫ –≤ \(\mathbb{R}^d\) —Å –ø–æ–º–æ—â—å—é –∞–ª–≥–æ—Ä–∏—Ç–º–∞ —Ç–∏–ø–∞ Grassberger‚ÄìProcaccia;[web:38][web:40][web:160]  
- –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–ª—É—á–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ D‚ÇÇ –∫–∞–∫ —ç—Ç–∞–ª–æ–Ω–Ω–æ–µ \(D_{\text{target}}\) –¥–ª—è —Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–æ–≥–æ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ç–æ—Ä–∞ Lingua GRA.

---

## üá∑–ì–£ 2. –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

- Python 3.10+
- PyTorch
- gensim (–¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ word2vec-–º–æ–¥–µ–ª–∏)
- numpy
- –≠—Ç–æ—Ç —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∫–∞–∫ –ø–∞–∫–µ—Ç (`pip install -e .`).

---

## üá∑–ì–£ 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤

1. –°–æ–±–µ—Ä–∏—Ç–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ—Ä–ø—É—Å (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞—É—á–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∏–ª–∏ —Å–º–µ—à–∞–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å).  
2. –û–±—É—á–∏—Ç–µ word2vec (–º–æ–∂–Ω–æ —Å –ø–æ–º–æ—â—å—é gensim, —Å–º. –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –∏ –ø—Ä–∏–º–µ—Ä—ã).[web:202][web:204]  
3. –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –º–æ–¥–µ–ª—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ word2vec (`.txt` –∏–ª–∏ –±–∏–Ω–∞—Ä–Ω—ã–π).

–ü—Ä–∏–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ word2vec (gensim):

```python
from gensim.models import Word2Vec
from gensim.utils import simple_preprocess

sentences = [simple_preprocess(line) for line in open("corpus_all.txt", encoding="utf-8")]
model = Word2Vec(
    sentences,
    vector_size=100,
    window=5,
    min_count=5,
    workers=4,
)
model.wv.save_word2vec_format("embeddings.vec", binary=False)
