"""
generate.py โ ะฟัะพััะฐั ะณะตะฝะตัะฐัะธั ัะตะบััะฐ ะฝะฐ ะฑะฐะทะต Lingua GRA

๐ท๐บ
ะกะบัะธะฟั ะดะตะผะพะฝัััะธััะตั:
- ะทะฐะณััะทะบั ะพะฑััะตะฝะฝะพะน ะผะพะดะตะปะธ Lingua GRA (ัะธะผะฒะพะปัะฝัะน + ัะตะผะฐะฝัะธัะตัะบะธะน ััะพะฒะฝะธ);
- ะฟะพัะฐะณะพะฒัั ะณะตะฝะตัะฐัะธั ะฟะพัะปะตะดะพะฒะฐัะตะปัะฝะพััะธ ัะพะบะตะฝะพะฒ;
- ะปะพะณะธัะพะฒะฐะฝะธะต ัะตะผะฐะฝัะธัะตัะบะพะณะพ ัะผะฑะตะดะดะธะฝะณะฐ ะธ ะพัะตะฝะบั ะตะณะพ D2 ะฟะพ ะผะธะฝะธ-ะฑะฐััั.

๐ฌ๐ง
This script demonstrates:
- loading a trained Lingua GRA model (symbolic + semantic levels);
- step-by-step token sequence generation;
- logging semantic embeddings and estimating their D2 on a mini-batch.
"""

from __future__ import annotations

import argparse
import logging
from typing import List

import torch
import torch.nn.functional as F

from lingua_gra.fractal_utils import correlation_dimension
from lingua_gra.language_levels import Level
from lingua_gra.training import LinguaGRAModel
from lingua_gra.utils import setup_logging, set_seed


def sample_next_token(logits: torch.Tensor, temperature: float = 1.0) -> int:
    """
    ๐ทะะฃ ะกัะผะฟะปะธัะพะฒะฐะฝะธะต ัะปะตะดัััะตะณะพ ัะพะบะตะฝะฐ ะธะท logits.

    ๐ฌ๐ง Sample next token from logits.
    """
    if temperature <= 0.0:
        return int(logits.argmax(dim=-1).item())
    probs = F.softmax(logits / temperature, dim=-1)
    return int(torch.multinomial(probs, 1).item())


def generate_sequence(
    model: LinguaGRAModel,
    start_tokens: List[int],
    max_len: int,
    temperature: float = 1.0,
    device: torch.device | None = None,
):
    """
    ๐ทะะฃ ะกะณะตะฝะตัะธัะพะฒะฐัั ะฟะพัะปะตะดะพะฒะฐัะตะปัะฝะพััั ัะพะบะตะฝะพะฒ, ะฝะฐัะธะฝะฐั ั start_tokens.

    ๐ฌ๐ง Generate a token sequence starting from start_tokens.
    """
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model.eval()

    tokens = torch.tensor(start_tokens, dtype=torch.long, device=device).unsqueeze(0)  # [1, T0]
    generated = start_tokens[:]

    with torch.no_grad():
        for _ in range(max_len - len(start_tokens)):
            h_sym, h_sym_proj, logits = model.forward_token_level(tokens)
            # logits: [B, V] โ ะธะฝัะตัะฟัะตัะธััะตะผ ะบะฐะบ ัะปะตะดัััะตะณะพ ัะพะบะตะฝะฐ ะดะปั ะฒัะตะณะพ ะบะพะฝัะตะบััะฐ
            next_id = sample_next_token(logits[0], temperature=temperature)
            generated.append(next_id)

            next_token = torch.tensor([[next_id]], dtype=torch.long, device=device)
            tokens = torch.cat([tokens, next_token], dim=1)

    return generated


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--checkpoint", type=str, required=False, help="Path to a trained Lingua GRA checkpoint (.pt).")
    parser.add_argument("--start-ids", type=str, default="1,2,3", help="Comma-separated list of start token ids.")
    parser.add_argument("--max-len", type=int, default=32)
    parser.add_argument("--temperature", type=float, default=1.0)
    parser.add_argument("--seed", type=int, default=42)
    args = parser.parse_args()

    setup_logging()
    logger = logging.getLogger("generate")
    set_seed(args.seed)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # ะะดะตัั ะฟัะตะดะฟะพะปะฐะณะฐะตััั, ััะพ ะฒั ะณะดะต-ัะพ ะพะฟัะตะดะตะปะธะปะธ ััะฝะบัะธั build_model_from_config
    # ะธะปะธ ะณััะทะธัะต ัะถะต ะณะพัะพะฒัะน LinguaGRAModel ะธะท ัะตะบะฟะพะธะฝัะฐ.
    #
    # ะัะธะผะตั (ะฟัะตะฒะดะพะบะพะด, ะทะฐะฒะธัะธั ะพั ะฒะฐัะตะณะพ ะฟัะพะตะบัะฐ):
    #
    # from lingua_gra.config import load_config_and_build_model
    # config, model = load_config_and_build_model("config.yaml")
    #
    # ะะดะตัั ะพััะฐะฒะธะผ ะทะฐะณะปััะบั:
    if args.checkpoint is None:
        raise RuntimeError("Checkpoint path must be provided to load a trained model.")

    checkpoint = torch.load(args.checkpoint, map_location=device)
    model: LinguaGRAModel = checkpoint["model"]  # ะฟัะตะดะฟะพะปะฐะณะฐะตะผ, ััะพ ัะฐะบ ัะพััะฐะฝะตะฝะพ
    model.to(device)

    start_ids = [int(x) for x in args.start_ids.split(",") if x.strip()]
    gen_ids = generate_sequence(
        model=model,
        start_tokens=start_ids,
        max_len=args.max_len,
        temperature=args.temperature,
        device=device,
    )

    logger.info(f"Generated token ids: {gen_ids}")

    # ะะพะฟะพะปะฝะธัะตะปัะฝะพ: ะพัะตะฝะบะฐ D2 ะดะปั ัะตะผะฐะฝัะธัะตัะบะธั ัะผะฑะตะดะดะธะฝะณะพะฒ ัะณะตะฝะตัะธัะพะฒะฐะฝะฝัั ะฟะพัะปะตะดะพะฒะฐัะตะปัะฝะพััะตะน
    # (ะธะณัััะตัะฝัะน ะฟัะธะผะตั: ะพะดะธะฝ ะฑะฐัั, ัััะตะดะฝะตะฝะธะต ัะพะบะตะฝะพะฒ โ ัะตะผะฐะฝัะธัะตัะบะธะน ะฒัะพะด).
    tokens = torch.tensor(gen_ids, dtype=torch.long, device=device).unsqueeze(0)
    with torch.no_grad():
        h_sym, h_sym_proj, _ = model.forward_token_level(tokens)
        # ะดะพะฟัััะธะผ, ัะตะผะฐะฝัะธัะตัะบะธะน ััะพะฒะตะฝั ะฑะตััั h_sym ะบะฐะบ ะฒัะพะด
        h_sem, h_sem_proj = model.forward_semantic_level(h_sym)

    # ััะพะฑั ะธะผะตัั ะพะฑะปะฐะบะพ ัะพัะตะบ, ะดัะฑะปะธััะตะผ ะฒะตะบัะพั ะธะปะธ ะธัะฟะพะปัะทัะตะผ ะฝะตัะบะพะปัะบะพ ะณะตะฝะตัะฐัะธะน;
    # ะทะดะตัั ะฟัะพััะพ ะดะตะปะฐะตะผ ะฒะธะด, ััะพ ั ะฝะฐั N ะบะพะฟะธะน
    embeddings = h_sem_proj.repeat(16, 1)  # [16, D]
    D2, _ = correlation_dimension(embeddings)
    logger.info(f"Estimated D2 for generated semantic embeddings (toy): {D2.item():.4f}")


if __name__ == "__main__":
    main()
